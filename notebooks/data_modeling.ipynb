{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling\n",
    "\n",
    "Now that we've cleaned and explored our data, we can start working on modeling it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from fbprophet import Prophet\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all of our cleaned .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/clean/full/dengue_features_train.csv')\n",
    "df_labels = pd.read_csv('../data/clean/full/dengue_features_train.csv')\n",
    "sj_features = pd.read_csv('../data/clean/sj/sj_train_features.csv')\n",
    "sj_labels = pd.read_csv('../data/clean/sj/sj_train_labels.csv')\n",
    "\n",
    "iq_features = pd.read_csv('../data/clean/iq/iq_train_features.csv')\n",
    "iq_labels = pd.read_csv('../data/clean/iq/iq_train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start playing around with algorithms and their hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sj -27.60013422946121 Lasso(alpha=100, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "      normalize=False, positive=False, precompute=False, random_state=None,\n",
      "      selection='cyclic', tol=0.0001, warm_start=False)\n",
      "sj -27.288571587403965 ElasticNet(alpha=100, copy_X=True, fit_intercept=True, l1_ratio=0.1,\n",
      "           max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "sj -30.038683611654715 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "enet = ElasticNet()\n",
    "reg = LinearRegression()\n",
    "\n",
    "lasso_params = {\n",
    "    'alpha':[1, 5, 10, 100],\n",
    "}\n",
    "\n",
    "enet_params = {\n",
    "    'alpha':[.1, 1, 5, 10, 100],\n",
    "    'l1_ratio':[.1, .5, .9],\n",
    "}\n",
    "\n",
    "for estimator, params in zip([lasso, enet, reg], [lasso_params, enet_params, {}]):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=params,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(sj_features.drop(['city', 'week_start_date'], axis=1), sj_labels['total_cases'])\n",
    "    print(sj_features['city'].unique()[0], grid_search.best_score_, grid_search.best_estimator_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the scores for ElasticNet, L2 Least Squares, and Linear Regression come out to 28, 27, and 30 respectively. We can also see that the estimators with complexity penalties find that a higher penalty leads to a better score. This, however, is slightly false. By visualizing the results we can see they are actually just regressing towards the mean of the `total_cases` column.\n",
    "\n",
    "# More than a Baseline\n",
    "\n",
    "Now that we have simple, interpretable models as our baseline, we can start to increase the complexity. In order to avoid the tendency for our models to pull to the mean, we'll use a `RandomizedGridSearch` instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.332693268328946"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBRegressor()\n",
    "\n",
    "sj_xgb_X = sj_xgb_y = pd.DataFrame()\n",
    "\n",
    "sj_xgb_X['ds'] = sj_features['week_start_date']\n",
    "sj_xgb_X['y'] = sj_labels['total_cases']\n",
    "sj_xgb_y['y'] = sj_labels['total_cases']\n",
    "\n",
    "\n",
    "kf = TimeSeriesSplit(n_splits=5)\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(sj_xgb_X):\n",
    "    prophet = Prophet(\n",
    "        growth = 'linear',\n",
    "        yearly_seasonality = 10,\n",
    "        weekly_seasonality = False,\n",
    "        daily_seasonality = False,\n",
    "        seasonality_mode = 'additive'\n",
    "    )\n",
    "    X_train, X_test = sj_xgb_X.iloc[train_index], sj_xgb_X.iloc[test_index]\n",
    "    prophet.fit(X_train)\n",
    "    p = prophet.predict(X_test)\n",
    "    scores.append(mean_absolute_error(p['yhat'], sj_xgb_X['y'].iloc[test_index]))\n",
    "\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
